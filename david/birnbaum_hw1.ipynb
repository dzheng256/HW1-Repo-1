{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1: Import external MULTEXT-East 1984 corpus\n",
    "\n",
    "David J. Birnbaum, djbpitt@pitt.edu, 2019-01-09\n",
    "\n",
    "## Metadata\n",
    "\n",
    "The MULTEXT-EAST (MTE) 1984 corpus contains the text of George Orwell’s _1984_, with linguistic annotation, in 12 languages. It is available as item #104 at http://www.nltk.org/nltk_data/. Documentation is at http://nl.ijs.si/ME/V4/ and, in more detail, https://www.clarin.si/repository/xmlui/handle/11356/1043. The principal developer/editor is Tomaž Erjavec, and the license is CC BY-NC-SA 4.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-assessment\n",
    "\n",
    "* **What it does:** Load an external corpus with MTE markup and examine different types of linguistic annotation.\n",
    "* **What I would like to do but don’t know how:** Understand why Bulgarian and Macedonian are broken, and what it would take to fix them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the corpus\n",
    "\n",
    "NLTK incorporates an MTECorpusReader class that can parse MTE (TEI P5) markup. API documentation is at https://www.nltk.org/api/nltk.corpus.reader.html#module-nltk.corpus.reader.mte. We”ll use pandas and numpy, so let’s import them up front, along with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument is the path to the corpus directory, the second is a file mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = nltk.corpus.reader.MTECorpusReader(r'data/mte_teip5', r'.*\\.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What methods does the corpus reader expose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_MTECorpusReader__fileids',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '_encoding',\n",
       " '_fileids',\n",
       " '_get_root',\n",
       " '_para_block_reader',\n",
       " '_root',\n",
       " '_sent_tokenizer',\n",
       " '_sep',\n",
       " '_tagset',\n",
       " '_word_tokenizer',\n",
       " 'abspath',\n",
       " 'abspaths',\n",
       " 'citation',\n",
       " 'encoding',\n",
       " 'ensure_loaded',\n",
       " 'fileids',\n",
       " 'lemma_paras',\n",
       " 'lemma_sents',\n",
       " 'lemma_words',\n",
       " 'license',\n",
       " 'open',\n",
       " 'paras',\n",
       " 'raw',\n",
       " 'readme',\n",
       " 'root',\n",
       " 'sents',\n",
       " 'tagged_paras',\n",
       " 'tagged_sents',\n",
       " 'tagged_words',\n",
       " 'unicode_repr',\n",
       " 'words']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What files are available in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = reader.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many files are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about just the monolingual corpus files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oana-cs.xml',\n",
       " 'oana-en.xml',\n",
       " 'oana-et.xml',\n",
       " 'oana-fa.xml',\n",
       " 'oana-hu.xml',\n",
       " 'oana-pl.xml',\n",
       " 'oana-ro.xml',\n",
       " 'oana-sk.xml',\n",
       " 'oana-sl.xml',\n",
       " 'oana-sr.xml']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = filter(lambda x: x.startswith(\"oana\") ,ids)\n",
    "# Bulgarian and Macedonian are broken (see below), so we’ll exclude them\n",
    "ids = filter(lambda x: x not in [\"oana-bg.xml\", \"oana-mk.xml\"], ids)\n",
    "ids = list(ids) # we have to listify this, because we’re going to reuse it, and a filter can’t be reused\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How may words in each version? Let’s use a pandas dataframe instead of a numpy array, since the datatypes (fileid, word count) are heterogeneous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fileid</th>\n",
       "      <th>Word count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oana-cs.xml</td>\n",
       "      <td>100366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oana-en.xml</td>\n",
       "      <td>118424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oana-et.xml</td>\n",
       "      <td>94898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oana-fa.xml</td>\n",
       "      <td>108427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oana-hu.xml</td>\n",
       "      <td>98426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>oana-pl.xml</td>\n",
       "      <td>97413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>oana-ro.xml</td>\n",
       "      <td>118325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>oana-sk.xml</td>\n",
       "      <td>102074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>oana-sl.xml</td>\n",
       "      <td>112278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>oana-sr.xml</td>\n",
       "      <td>104290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Fileid  Word count\n",
       "0  oana-cs.xml      100366\n",
       "1  oana-en.xml      118424\n",
       "2  oana-et.xml       94898\n",
       "3  oana-fa.xml      108427\n",
       "4  oana-hu.xml       98426\n",
       "5  oana-pl.xml       97413\n",
       "6  oana-ro.xml      118325\n",
       "7  oana-sk.xml      102074\n",
       "8  oana-sl.xml      112278\n",
       "9  oana-sr.xml      104290"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc = map(lambda x: len(reader.words(x)), ids)\n",
    "df = pd.DataFrame(list(zip(ids, wc)), columns = [\"Fileid\", \"Word count\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the Fileid as the index and the word count as the value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fileid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>oana-cs.xml</th>\n",
       "      <td>100366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oana-en.xml</th>\n",
       "      <td>118424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oana-et.xml</th>\n",
       "      <td>94898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oana-fa.xml</th>\n",
       "      <td>108427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oana-hu.xml</th>\n",
       "      <td>98426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oana-pl.xml</th>\n",
       "      <td>97413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oana-ro.xml</th>\n",
       "      <td>118325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oana-sk.xml</th>\n",
       "      <td>102074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oana-sl.xml</th>\n",
       "      <td>112278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oana-sr.xml</th>\n",
       "      <td>104290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word count\n",
       "Fileid                 \n",
       "oana-cs.xml      100366\n",
       "oana-en.xml      118424\n",
       "oana-et.xml       94898\n",
       "oana-fa.xml      108427\n",
       "oana-hu.xml       98426\n",
       "oana-pl.xml       97413\n",
       "oana-ro.xml      118325\n",
       "oana-sk.xml      102074\n",
       "oana-sl.xml      112278\n",
       "oana-sr.xml      104290"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_index(\"Fileid\", inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the longest in word count? The shortest? What’s the mean word count? Pandas can provide max, min, and mean, along with other descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>105492.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8520.702703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>94898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>98911.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>103182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>111315.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>118424.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word count\n",
       "count      10.000000\n",
       "mean   105492.100000\n",
       "std      8520.702703\n",
       "min     94898.000000\n",
       "25%     98911.000000\n",
       "50%    103182.000000\n",
       "75%    111315.250000\n",
       "max    118424.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be more useful to report the fileid along with the value, though. Let’s try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word count    oana-en.xml\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! All we want is the value, so use a numerical index to get rid of the rest of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest file is oana-en.xml of length 118424 ; the shortest is oana-et.xml of length 94898 ; and the average length is 105492.1\n"
     ]
    }
   ],
   "source": [
    "longest_id = df.idxmax()[0]\n",
    "longest = df.max()[0]\n",
    "shortest_id = df.idxmin()[0]\n",
    "shortest = df.min()[0]\n",
    "average = df.mean()[0]\n",
    "print(\"The longest file is\", longest_id, \"of length\", longest, \"; the shortest is\",\n",
    "     shortest_id, \"of length\", shortest, \"; and the average length is\", average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use `numpy` methods, just because? A pandas series (in this case, a column of values in a one-column dataframe) is of type numpy.ndarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can use numpy methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest text is of length 118424 , the shortest is of length 94898 and the average if of length 105492.1\n"
     ]
    }
   ],
   "source": [
    "s = df.values\n",
    "longest = np.max(s)\n",
    "shortest = np.min(s)\n",
    "average = np.mean(s)\n",
    "print(\"The longest text is of length\", longest, \", the shortest is of length\", shortest, \n",
    "      \"and the average if of length\", average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking closely at one version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the shape of the Slovak version ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6354 sentences, 1359 paragraphs, and 102074 words in the Slovak version.\n"
     ]
    }
   ],
   "source": [
    "sents_count = len(reader.sents('oana-sk.xml'))\n",
    "paras_count = len(reader.paras('oana-sk.xml'))\n",
    "words_count = len(reader.words('oana-sk.xml'))\n",
    "print('There are', sents_count, 'sentences,', paras_count, \n",
    "      'paragraphs, and', words_count,'words in the Slovak version.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a list of word-tokenized sentences ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bol',\n",
       "  'jasný',\n",
       "  ',',\n",
       "  'ale',\n",
       "  'chladný',\n",
       "  'aprílový',\n",
       "  'deň',\n",
       "  'a',\n",
       "  'hodiny',\n",
       "  'odbíjali',\n",
       "  'trinástu',\n",
       "  '.'],\n",
       " ['S',\n",
       "  'bradou',\n",
       "  'pritlačenou',\n",
       "  'na',\n",
       "  'prsia',\n",
       "  ',',\n",
       "  'aby',\n",
       "  'sa',\n",
       "  'chránil',\n",
       "  'pred',\n",
       "  'dotieravým',\n",
       "  'vetrom',\n",
       "  ',',\n",
       "  'Winston',\n",
       "  'Smith',\n",
       "  'prekĺzol',\n",
       "  'rýchlo',\n",
       "  'cez',\n",
       "  'sklené',\n",
       "  'dvere',\n",
       "  'na',\n",
       "  'sídlisku',\n",
       "  'Víťazstvo',\n",
       "  ',',\n",
       "  'nie',\n",
       "  'však',\n",
       "  'tak',\n",
       "  'rýchlo',\n",
       "  ',',\n",
       "  'aby',\n",
       "  'zabránil',\n",
       "  'zvírenému',\n",
       "  'piesku',\n",
       "  'a',\n",
       "  'prachu',\n",
       "  'vniknúť',\n",
       "  'dnu',\n",
       "  's',\n",
       "  'ním',\n",
       "  '.'],\n",
       " ['V',\n",
       "  'chodbe',\n",
       "  'páchla',\n",
       "  'varená',\n",
       "  'kapusta',\n",
       "  'a',\n",
       "  'staré',\n",
       "  'handrové',\n",
       "  'rohožky',\n",
       "  '.']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_sents = reader.sents(['oana-sk.xml'])\n",
    "sk_sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatized, too ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Bol', 'byť'),\n",
       "  ('jasný', 'jasný'),\n",
       "  (',', ''),\n",
       "  ('ale', 'ale'),\n",
       "  ('chladný', 'chladný'),\n",
       "  ('aprílový', 'aprílový'),\n",
       "  ('deň', 'deň'),\n",
       "  ('a', 'a'),\n",
       "  ('hodiny', 'hodiny'),\n",
       "  ('odbíjali', 'odbíjať'),\n",
       "  ('trinástu', 'trinásty'),\n",
       "  ('.', '')],\n",
       " [('S', 's'),\n",
       "  ('bradou', 'brada'),\n",
       "  ('pritlačenou', 'pritlačený'),\n",
       "  ('na', 'na'),\n",
       "  ('prsia', 'prsia'),\n",
       "  (',', ''),\n",
       "  ('aby', 'aby'),\n",
       "  ('sa', 'sa'),\n",
       "  ('chránil', 'chrániť'),\n",
       "  ('pred', 'pred'),\n",
       "  ('dotieravým', 'dotieravý'),\n",
       "  ('vetrom', 'vietor'),\n",
       "  (',', ''),\n",
       "  ('Winston', 'winston'),\n",
       "  ('Smith', 'smith'),\n",
       "  ('prekĺzol', 'prekĺznuť'),\n",
       "  ('rýchlo', 'rýchlo'),\n",
       "  ('cez', 'cez'),\n",
       "  ('sklené', 'sklený'),\n",
       "  ('dvere', 'dvere'),\n",
       "  ('na', 'na'),\n",
       "  ('sídlisku', 'sídlisko'),\n",
       "  ('Víťazstvo', 'víťazstvo'),\n",
       "  (',', ''),\n",
       "  ('nie', 'nie'),\n",
       "  ('však', 'však'),\n",
       "  ('tak', 'tak'),\n",
       "  ('rýchlo', 'rýchlo'),\n",
       "  (',', ''),\n",
       "  ('aby', 'aby'),\n",
       "  ('zabránil', 'zabrániť'),\n",
       "  ('zvírenému', 'zvírený'),\n",
       "  ('piesku', 'piesok'),\n",
       "  ('a', 'a'),\n",
       "  ('prachu', 'prach'),\n",
       "  ('vniknúť', 'vniknúť'),\n",
       "  ('dnu', 'dnu'),\n",
       "  ('s', 's'),\n",
       "  ('ním', 'on'),\n",
       "  ('.', '')],\n",
       " [('V', 'v'),\n",
       "  ('chodbe', 'chodba'),\n",
       "  ('páchla', 'páchnuť'),\n",
       "  ('varená', 'varený'),\n",
       "  ('kapusta', 'kapusta'),\n",
       "  ('a', 'a'),\n",
       "  ('staré', 'starý'),\n",
       "  ('handrové', 'handrový'),\n",
       "  ('rohožky', 'rohožka'),\n",
       "  ('.', '')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_lemma_sents = reader.lemma_sents(['oana-sk.xml'])\n",
    "sk_lemma_sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences with morphological tagging are also available ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Bol', '#Vcps-sm-n-----p'),\n",
       "  ('jasný', '#Afpmsn'),\n",
       "  (',', ''),\n",
       "  ('ale', '#Cs'),\n",
       "  ('chladný', '#Afpmsn'),\n",
       "  ('aprílový', '#Afpmsn'),\n",
       "  ('deň', '#Ncmsn'),\n",
       "  ('a', '#Cc'),\n",
       "  ('hodiny', '#Ncfpn'),\n",
       "  ('odbíjali', '#Vmps-pf-n-----p'),\n",
       "  ('trinástu', '#Mofsal--f'),\n",
       "  ('.', '')],\n",
       " [('S', '#Spsi'),\n",
       "  ('bradou', '#Ncfsi'),\n",
       "  ('pritlačenou', '#Afpfsi'),\n",
       "  ('na', '#Spsa'),\n",
       "  ('prsia', '#Ncnpa'),\n",
       "  (',', ''),\n",
       "  ('aby', '#Cs'),\n",
       "  ('sa', '#Px---a--ypn'),\n",
       "  ('chránil', '#Vmps-sm-n-----p'),\n",
       "  ('pred', '#Spsi'),\n",
       "  ('dotieravým', '#Afpmsi'),\n",
       "  ('vetrom', '#Ncmsi'),\n",
       "  (',', ''),\n",
       "  ('Winston', '#Npmsn--y'),\n",
       "  ('Smith', '#Npmsn--y'),\n",
       "  ('prekĺzol', '#Vmps-sm-n-----e'),\n",
       "  ('rýchlo', '#R-p'),\n",
       "  ('cez', '#Spsa'),\n",
       "  ('sklené', '#Afpfpa'),\n",
       "  ('dvere', '#Ncfpa'),\n",
       "  ('na', '#Spsl'),\n",
       "  ('sídlisku', '#Ncnsl'),\n",
       "  ('Víťazstvo', '#Ncnsn'),\n",
       "  (',', ''),\n",
       "  ('nie', '#Q'),\n",
       "  ('však', '#Q'),\n",
       "  ('tak', '#R-p'),\n",
       "  ('rýchlo', '#R-p'),\n",
       "  (',', ''),\n",
       "  ('aby', '#Cs'),\n",
       "  ('zabránil', '#Vmps-sm-n-----e'),\n",
       "  ('zvírenému', '#Afpmsd'),\n",
       "  ('piesku', '#Ncmsd'),\n",
       "  ('a', '#Cc'),\n",
       "  ('prachu', '#Ncmsd'),\n",
       "  ('vniknúť', '#Vmn-----n-----e'),\n",
       "  ('dnu', '#R-p'),\n",
       "  ('s', '#Spsi'),\n",
       "  ('ním', '#Pp3msi--n-n'),\n",
       "  ('.', '')],\n",
       " [('V', '#Spsl'),\n",
       "  ('chodbe', '#Ncfsl'),\n",
       "  ('páchla', '#Vmps-sf-n-----p'),\n",
       "  ('varená', '#Afpfsn'),\n",
       "  ('kapusta', '#Ncfsn'),\n",
       "  ('a', '#Cc'),\n",
       "  ('staré', '#Afpfpn'),\n",
       "  ('handrové', '#Afpfpn'),\n",
       "  ('rohožky', '#Ncfpn'),\n",
       "  ('.', '')]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_tagged_sents = reader.tagged_sents(['oana-sk.xml'])\n",
    "sk_tagged_sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovery \n",
    "\n",
    "Bulgarian seems to be broken ... eek!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "concat() expects at least one object!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-faca1c60701c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbg_tagged_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'oana-bg.xml'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbg_tagged_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/mte.py\u001b[0m in \u001b[0;36mtagged_sents\u001b[0;34m(self, fileids, tagset, tags)\u001b[0m\n\u001b[1;32m    369\u001b[0m                         \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     )\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                 ]\n\u001b[1;32m    373\u001b[0m             )\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(docs)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'concat() expects at least one object!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: concat() expects at least one object!"
     ]
    }
   ],
   "source": [
    "bg_tagged_sents = reader.tagged_sents(['oana-bg.xml'])\n",
    "bg_tagged_sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently this is because Bulgarian and Macedonian are said not to be TEI-P5 conformant (although they validate against the Relax NG schema that accompanies the distribution). The documentation at http://www.nltk.org/_modules/nltk/corpus/reader/mte.html includes a hint at:\n",
    "\n",
    "```python\n",
    "# filter multext-east sourcefiles that are not compatible to the teip5 specification\n",
    "        fileids = filter(lambda x: x not in [\"oana-bg.xml\", \"oana-mk.xml\"], fileids)\n",
    "```\n",
    "\n",
    "This seems to say that the Bulgarian and Macedonian filenames should not be returned by the `.fileids()` method, which is peculiar, since when we ran `reader.fileids()` above, they were. The XML markup is not uniform within the corpus (for example, only Bulgarian and English contain `@function` attributes), so perhaps the answer lies there, but that’s a topic best explored in an XML environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
